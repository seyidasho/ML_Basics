{"title":"Anomaly Detection","markdown":{"yaml":{"title":"Anomaly Detection","author":"Seyi Dasho","date":"December 6 2023","format":{"html":{"code-fold":true}},"jupyter":"python3"},"headingText":"Generate sample data","containsRefs":false,"markdown":"\n\n**Anomaly/outlier detection**<br>\ni.\tAnomaly detection refers to identifying rare items, events or observations that are significantly different from the majority of the data. These outliers or anomalies may be indicative of some issue or abnormal behavior.<br>\nii.\tGoal is to detect anomalies with high accuracy while minimizing false positives.<br>\niii.\tApplications include fraud detection, network intrusion, breakdown detection in manufacturing, etc.<br>\n**Common approaches:** <br>\ni.\tStatistical methods: Declare points outliers if they are far from mean/median or in the tails of a distribution.<br>\nii.\tDistance-based: Find points with low nearest neighbor density as outliers.<br>\niii.\tClassification-based: Train binary classifiers to distinguish between normal and outlier classes.<br>\niv.\tClustering: Identify small clusters or points far from clusters as anomalous.<br>\n\n•\tUnsupervised methods like statistical metrics, isolation forests and local outlier factor do not require labeled data.<br>\n•\tSupervised models like SVM, neural networks can be trained if labeled examples of anomalies exist.<br>\n•\tKey challenges are class imbalance (far more normal points than anomalies), obtaining labels, and defining anomalies for complex data.<br>\n•\tEvaluation metrics include precision, recall, F1-score. Cross-validation helps avoid overfitting.<br>\n•\tOverall, anomaly detection is critical for identifying irregularities, errors, novelties and outliers that could indicate important events or issues.<br>\n\n**Example using Local Outlier Factor**\n\n```{python}\n#| tags: []\nfrom sklearn.neighbors import LocalOutlierFactor\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = 0.3 * np.random.randn(100, 2)\nX = np.r_[X + 2, X - 2]\n\n# \n\n# Fit LOF model\nclf = LocalOutlierFactor()\ny_pred = clf.fit_predict(X)\n\n# Identify outliers\noutlier_idx = np.where(y_pred == -1)\n\n# Plot data\nplt.scatter(X[:,0], X[:,1], s=50)\nplt.scatter(X[outlier_idx,0], X[outlier_idx,1], color='r', s=100)\nplt.title(\"Local Outlier Factor\")\nplt.show()\n```\n\n•\tThis fits a LOF model to randomly generated sample data, makes predictions to identify outliers, and plots the outliers in red.<br>\n•\tLOF is an unsupervised anomaly detection method that works well with low dimensional data. The key steps - fit model, predict outliers, evaluate results - demonstrate the basic workflow for applying anomaly detection in Python.<br>\n\n**Example using Isolation Forest**\n\n```{python}\n#| tags: []\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport warnings\nwarnings.filterwarnings('ignore')\n# Load the wine dataset from Scikit-learn\nwine = load_wine()\nX = wine.data\ny = wine.target\n\n# Add outliers to the dataset (for demonstration purposes)\noutliers = np.array([[14, 2, 2, 80, 1, 3, 1, 1, 5, 1, 1, 1, 1000],  # Adding an outlier to the first sample\n                    [12, 1, 2, 10, 2, 2, 1, 2, 3, 1, 2, 1, 50]])  # Adding an outlier to the second sample\nX_with_outliers = np.vstack([X, outliers])\ny_with_outliers = np.concatenate([y, [-1, -1]])  # Adding labels for outliers\n\n# Fit the Isolation Forest model\nmodel = IsolationForest(contamination=0.05)  # Adjust the contamination parameter as needed\nmodel.fit(X_with_outliers)\n\n# Predict anomalies\npredictions = model.predict(X_with_outliers)\n\n# Evaluate the model\nprecision = precision_score(y_with_outliers, predictions, pos_label=-1, average='weighted')\nrecall = recall_score(y_with_outliers, predictions, pos_label=-1, average='weighted')\nf1 = f1_score(y_with_outliers, predictions, pos_label=-1, average='weighted')\n\nprint(f'Precision: {precision:.2f}')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1 Score: {f1:.2f}')\n```\n\n```{python}\n#| tags: []\n# Plot the results\nplt.figure(figsize=(10, 6))\n\n# Plot normal data points\nplt.scatter(X[:, 0], X[:, 1], c='blue', label='Normal Data')\n\n# Plot outliers\nplt.scatter(outliers[:, 0], outliers[:, 1], c='red', label='Outliers', marker='x', s=100)\n\n# Highlight predicted anomalies\nplt.scatter(X_with_outliers[predictions == -1, 0], X_with_outliers[predictions == -1, 1],\n            c='yellow', label='Predicted Anomalies', marker='o')\n\nplt.title('Isolation Forest Anomaly Detection on Scikit-learn Wine Data (with Outliers)')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n```\n\n\n","srcMarkdownNoYaml":"\n\n**Anomaly/outlier detection**<br>\ni.\tAnomaly detection refers to identifying rare items, events or observations that are significantly different from the majority of the data. These outliers or anomalies may be indicative of some issue or abnormal behavior.<br>\nii.\tGoal is to detect anomalies with high accuracy while minimizing false positives.<br>\niii.\tApplications include fraud detection, network intrusion, breakdown detection in manufacturing, etc.<br>\n**Common approaches:** <br>\ni.\tStatistical methods: Declare points outliers if they are far from mean/median or in the tails of a distribution.<br>\nii.\tDistance-based: Find points with low nearest neighbor density as outliers.<br>\niii.\tClassification-based: Train binary classifiers to distinguish between normal and outlier classes.<br>\niv.\tClustering: Identify small clusters or points far from clusters as anomalous.<br>\n\n•\tUnsupervised methods like statistical metrics, isolation forests and local outlier factor do not require labeled data.<br>\n•\tSupervised models like SVM, neural networks can be trained if labeled examples of anomalies exist.<br>\n•\tKey challenges are class imbalance (far more normal points than anomalies), obtaining labels, and defining anomalies for complex data.<br>\n•\tEvaluation metrics include precision, recall, F1-score. Cross-validation helps avoid overfitting.<br>\n•\tOverall, anomaly detection is critical for identifying irregularities, errors, novelties and outliers that could indicate important events or issues.<br>\n\n**Example using Local Outlier Factor**\n\n```{python}\n#| tags: []\nfrom sklearn.neighbors import LocalOutlierFactor\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate sample data\nX = 0.3 * np.random.randn(100, 2)\nX = np.r_[X + 2, X - 2]\n\n# \n\n# Fit LOF model\nclf = LocalOutlierFactor()\ny_pred = clf.fit_predict(X)\n\n# Identify outliers\noutlier_idx = np.where(y_pred == -1)\n\n# Plot data\nplt.scatter(X[:,0], X[:,1], s=50)\nplt.scatter(X[outlier_idx,0], X[outlier_idx,1], color='r', s=100)\nplt.title(\"Local Outlier Factor\")\nplt.show()\n```\n\n•\tThis fits a LOF model to randomly generated sample data, makes predictions to identify outliers, and plots the outliers in red.<br>\n•\tLOF is an unsupervised anomaly detection method that works well with low dimensional data. The key steps - fit model, predict outliers, evaluate results - demonstrate the basic workflow for applying anomaly detection in Python.<br>\n\n**Example using Isolation Forest**\n\n```{python}\n#| tags: []\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport warnings\nwarnings.filterwarnings('ignore')\n# Load the wine dataset from Scikit-learn\nwine = load_wine()\nX = wine.data\ny = wine.target\n\n# Add outliers to the dataset (for demonstration purposes)\noutliers = np.array([[14, 2, 2, 80, 1, 3, 1, 1, 5, 1, 1, 1, 1000],  # Adding an outlier to the first sample\n                    [12, 1, 2, 10, 2, 2, 1, 2, 3, 1, 2, 1, 50]])  # Adding an outlier to the second sample\nX_with_outliers = np.vstack([X, outliers])\ny_with_outliers = np.concatenate([y, [-1, -1]])  # Adding labels for outliers\n\n# Fit the Isolation Forest model\nmodel = IsolationForest(contamination=0.05)  # Adjust the contamination parameter as needed\nmodel.fit(X_with_outliers)\n\n# Predict anomalies\npredictions = model.predict(X_with_outliers)\n\n# Evaluate the model\nprecision = precision_score(y_with_outliers, predictions, pos_label=-1, average='weighted')\nrecall = recall_score(y_with_outliers, predictions, pos_label=-1, average='weighted')\nf1 = f1_score(y_with_outliers, predictions, pos_label=-1, average='weighted')\n\nprint(f'Precision: {precision:.2f}')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1 Score: {f1:.2f}')\n```\n\n```{python}\n#| tags: []\n# Plot the results\nplt.figure(figsize=(10, 6))\n\n# Plot normal data points\nplt.scatter(X[:, 0], X[:, 1], c='blue', label='Normal Data')\n\n# Plot outliers\nplt.scatter(outliers[:, 0], outliers[:, 1], c='red', label='Outliers', marker='x', s=100)\n\n# Highlight predicted anomalies\nplt.scatter(X_with_outliers[predictions == -1, 0], X_with_outliers[predictions == -1, 1],\n            c='yellow', label='Predicted Anomalies', marker='o')\n\nplt.title('Isolation Forest Anomaly Detection on Scikit-learn Wine Data (with Outliers)')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n```\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"Anomaly_Detection.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","title":"Anomaly Detection","author":"Seyi Dasho","date":"December 6 2023","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}