{"title":"Linear and Non-linear Regression","markdown":{"yaml":{"title":"Linear and Non-linear Regression","author":"Seyi Dasho","date":"December 6 2023","format":{"html":{"code-fold":true}},"jupyter":"python3"},"headingText":"Load the diabetes dataset","containsRefs":false,"markdown":"\n\n**Linear and Nonlinear Regression** <br>\nHere is an overview of linear and nonlinear regression in machine learning:<br>\n\n**Linear Regression:** <br>\n- Models the relationship between dependent and independent variables as a linear function. <br>\n- Takes the form:<br> \n  $$y = mx + b$$ <br>\n  $$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\epsilon$$\n\nWhere:\n\n    $y$ is the response/dependent variable\n    $\\beta_0$ is the intercept term\n    $\\beta_1,...,\\beta_p$ are the regression coefficients\n    $x_1,...,x_p$ are the predictor/independent variables\n    $\\epsilon$ is the model error term\n\n- Simple and fast to implement, good exploratory technique. <br>\n- Prone to underfitting and cannot capture nonlinear patterns. <br>\n- Examples: Simple, multiple, polynomial regression. <br>\n\n**Nonlinear Regression:** \n- Models the relationship between variables as a nonlinear function. More flexible and can fit complex patterns.<br>\n- Examples include polynomial, logistic, exponential functions. <br>\n- Takes more computational power and data to fit properly. <br>\n- Examples: Polynomial regression, support vector regression. <br>\n\n- Kernel trick can make linear regression algorithms fit nonlinear patterns. Maps data to higher dimensions.<br>\n- Regularization helps prevent overfitting for both linear and nonlinear models. Penalizes model complexity.<br>\n- Model evaluation metrics like R-squared, MSE help assess fit. Cross-validation reduces overfitting.<br>\n- Feature engineering creates meaningful nonlinear transformations of data for modeling.<br>\nOverall, linear models provide a fast and simple approach, while nonlinear models can capture more complex relationships but require more data and computing power to train effectively. The choice depends on the specific dataset and use case.\n\n```{python}\n#| tags: []\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ndiabetes = load_diabetes()\nX, y = diabetes.data, diabetes.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a linear regression model\nmodel = LinearRegression()\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R-squared: {r2:.2f}')\n\n# Plot the predicted vs. actual values\nplt.scatter(y_test, y_pred)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Linear Regression: Actual vs. Predicted Values')\nplt.show()\n```\n\n```{python}\n#| tags: []\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Apply Polynomial Regression\ndegree = 2  # Adjust the degree as needed\npoly_features = PolynomialFeatures(degree=degree)\nX_poly_train = poly_features.fit_transform(X_train)\nX_poly_test = poly_features.transform(X_test)\n\nmodel = LinearRegression()\nmodel.fit(X_poly_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_poly_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R-squared: {r2:.2f}')\n\n# Plot the results\nsorted_indices = X_test.argsort(axis=0).flatten()    \nplt.scatter(y_test, y_pred, color='red', label='Predicted Data (Polynomial Regression)')\nplt.xlabel('Feature')\nplt.ylabel('Target')\nplt.legend()\nplt.show()\n```\n\n\n","srcMarkdownNoYaml":"\n\n**Linear and Nonlinear Regression** <br>\nHere is an overview of linear and nonlinear regression in machine learning:<br>\n\n**Linear Regression:** <br>\n- Models the relationship between dependent and independent variables as a linear function. <br>\n- Takes the form:<br> \n  $$y = mx + b$$ <br>\n  $$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\epsilon$$\n\nWhere:\n\n    $y$ is the response/dependent variable\n    $\\beta_0$ is the intercept term\n    $\\beta_1,...,\\beta_p$ are the regression coefficients\n    $x_1,...,x_p$ are the predictor/independent variables\n    $\\epsilon$ is the model error term\n\n- Simple and fast to implement, good exploratory technique. <br>\n- Prone to underfitting and cannot capture nonlinear patterns. <br>\n- Examples: Simple, multiple, polynomial regression. <br>\n\n**Nonlinear Regression:** \n- Models the relationship between variables as a nonlinear function. More flexible and can fit complex patterns.<br>\n- Examples include polynomial, logistic, exponential functions. <br>\n- Takes more computational power and data to fit properly. <br>\n- Examples: Polynomial regression, support vector regression. <br>\n\n- Kernel trick can make linear regression algorithms fit nonlinear patterns. Maps data to higher dimensions.<br>\n- Regularization helps prevent overfitting for both linear and nonlinear models. Penalizes model complexity.<br>\n- Model evaluation metrics like R-squared, MSE help assess fit. Cross-validation reduces overfitting.<br>\n- Feature engineering creates meaningful nonlinear transformations of data for modeling.<br>\nOverall, linear models provide a fast and simple approach, while nonlinear models can capture more complex relationships but require more data and computing power to train effectively. The choice depends on the specific dataset and use case.\n\n```{python}\n#| tags: []\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the diabetes dataset\ndiabetes = load_diabetes()\nX, y = diabetes.data, diabetes.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a linear regression model\nmodel = LinearRegression()\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R-squared: {r2:.2f}')\n\n# Plot the predicted vs. actual values\nplt.scatter(y_test, y_pred)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Linear Regression: Actual vs. Predicted Values')\nplt.show()\n```\n\n```{python}\n#| tags: []\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Apply Polynomial Regression\ndegree = 2  # Adjust the degree as needed\npoly_features = PolynomialFeatures(degree=degree)\nX_poly_train = poly_features.fit_transform(X_train)\nX_poly_test = poly_features.transform(X_test)\n\nmodel = LinearRegression()\nmodel.fit(X_poly_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_poly_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R-squared: {r2:.2f}')\n\n# Plot the results\nsorted_indices = X_test.argsort(axis=0).flatten()    \nplt.scatter(y_test, y_pred, color='red', label='Predicted Data (Polynomial Regression)')\nplt.xlabel('Feature')\nplt.ylabel('Target')\nplt.legend()\nplt.show()\n```\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"L_NL_Regression.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","title":"Linear and Non-linear Regression","author":"Seyi Dasho","date":"December 6 2023","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}