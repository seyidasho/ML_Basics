{
  "hash": "c3d7e381ac6813be0e569ef6ca488da4",
  "result": {
    "markdown": "---\ntitle: Anomaly Detection\nauthor: Seyi Dasho\ndate: December 6 2023\nformat:\n  html:\n    code-fold: true\n---\n\n**Anomaly/outlier detection**<br>\ni.\tAnomaly detection refers to identifying rare items, events or observations that are significantly different from the majority of the data. These outliers or anomalies may be indicative of some issue or abnormal behavior.<br>\nii.\tGoal is to detect anomalies with high accuracy while minimizing false positives.<br>\niii.\tApplications include fraud detection, network intrusion, breakdown detection in manufacturing, etc.<br>\n**Common approaches:** <br>\ni.\tStatistical methods: Declare points outliers if they are far from mean/median or in the tails of a distribution.<br>\nii.\tDistance-based: Find points with low nearest neighbor density as outliers.<br>\niii.\tClassification-based: Train binary classifiers to distinguish between normal and outlier classes.<br>\niv.\tClustering: Identify small clusters or points far from clusters as anomalous.<br>\n\n•\tUnsupervised methods like statistical metrics, isolation forests and local outlier factor do not require labeled data.<br>\n•\tSupervised models like SVM, neural networks can be trained if labeled examples of anomalies exist.<br>\n•\tKey challenges are class imbalance (far more normal points than anomalies), obtaining labels, and defining anomalies for complex data.<br>\n•\tEvaluation metrics include precision, recall, F1-score. Cross-validation helps avoid overfitting.<br>\n•\tOverall, anomaly detection is critical for identifying irregularities, errors, novelties and outliers that could indicate important events or issues.<br>\n\n**Example using Local Outlier Factor**\n\n::: {.cell tags='[]' execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.neighbors import LocalOutlierFactor\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate sample data\nX = 0.3 * np.random.randn(100, 2)\nX = np.r_[X + 2, X - 2]\n\n# \n\n# Fit LOF model\nclf = LocalOutlierFactor()\ny_pred = clf.fit_predict(X)\n\n# Identify outliers\noutlier_idx = np.where(y_pred == -1)\n\n# Plot data\nplt.scatter(X[:,0], X[:,1], s=50)\nplt.scatter(X[outlier_idx,0], X[outlier_idx,1], color='r', s=100)\nplt.title(\"Local Outlier Factor\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Anomaly_Detection_files/figure-html/cell-2-output-1.png){width=569 height=431}\n:::\n:::\n\n\n•\tThis fits a LOF model to randomly generated sample data, makes predictions to identify outliers, and plots the outliers in red.<br>\n•\tLOF is an unsupervised anomaly detection method that works well with low dimensional data. The key steps - fit model, predict outliers, evaluate results - demonstrate the basic workflow for applying anomaly detection in Python.<br>\n\n**Example using Isolation Forest**\n\n::: {.cell tags='[]' execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport warnings\nwarnings.filterwarnings('ignore')\n# Load the wine dataset from Scikit-learn\nwine = load_wine()\nX = wine.data\ny = wine.target\n\n# Add outliers to the dataset (for demonstration purposes)\noutliers = np.array([[14, 2, 2, 80, 1, 3, 1, 1, 5, 1, 1, 1, 1000],  # Adding an outlier to the first sample\n                    [12, 1, 2, 10, 2, 2, 1, 2, 3, 1, 2, 1, 50]])  # Adding an outlier to the second sample\nX_with_outliers = np.vstack([X, outliers])\ny_with_outliers = np.concatenate([y, [-1, -1]])  # Adding labels for outliers\n\n# Fit the Isolation Forest model\nmodel = IsolationForest(contamination=0.05)  # Adjust the contamination parameter as needed\nmodel.fit(X_with_outliers)\n\n# Predict anomalies\npredictions = model.predict(X_with_outliers)\n\n# Evaluate the model\nprecision = precision_score(y_with_outliers, predictions, pos_label=-1, average='weighted')\nrecall = recall_score(y_with_outliers, predictions, pos_label=-1, average='weighted')\nf1 = f1_score(y_with_outliers, predictions, pos_label=-1, average='weighted')\n\nprint(f'Precision: {precision:.2f}')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1 Score: {f1:.2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrecision: 0.16\nRecall: 0.38\nF1 Score: 0.22\n```\n:::\n:::\n\n\n::: {.cell tags='[]' execution_count=3}\n``` {.python .cell-code}\n# Plot the results\nplt.figure(figsize=(10, 6))\n\n# Plot normal data points\nplt.scatter(X[:, 0], X[:, 1], c='blue', label='Normal Data')\n\n# Plot outliers\nplt.scatter(outliers[:, 0], outliers[:, 1], c='red', label='Outliers', marker='x', s=100)\n\n# Highlight predicted anomalies\nplt.scatter(X_with_outliers[predictions == -1, 0], X_with_outliers[predictions == -1, 1],\n            c='yellow', label='Predicted Anomalies', marker='o')\n\nplt.title('Isolation Forest Anomaly Detection on Scikit-learn Wine Data (with Outliers)')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Anomaly_Detection_files/figure-html/cell-4-output-1.png){width=811 height=523}\n:::\n:::\n\n\n",
    "supporting": [
      "Anomaly_Detection_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}