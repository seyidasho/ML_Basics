{
  "hash": "1fc82dee9e560da669a5db0202013828",
  "result": {
    "markdown": "---\ntitle: Probability Theory and Random Variable\nauthor: Seyi Dasho\ndate: December 6 2023\nformat:\n  html:\n    code-fold: true\n---\n\n**Probability Theory**<br>\nProbability theory provides the mathematical framework for modeling uncertainty and randomness. It allows us to calculate probabilities, make predictions, and quantify the likelihood of different outcomes.\n$$\\displaystyle \\operatorname P(A\\vert B)={\\frac {P(B\\vert A)P(A)}{P(B)}}$$\n\n**Random Variables**<br>\nRandom variables are key components of probability theory. A random variable represents an outcome that has some randomness or uncertainty associated with it. For example, the weather on a given day can be modeled as a random variable.<br>\nThe probability that random variable **X** takes on a value in a measurable set S âŠ† E  is written as \n$${\\displaystyle \\operatorname {P} (X\\in S)=\\operatorname {P} (\\{\\omega \\in \\Omega \\mid X(\\omega )\\in S\\})}$$\n**Application in Machine Learning**<br>\nMany machine learning algorithms rely on probability and statistics to model the uncertainty in the data. For example, Naive Bayes classifiers use Bayes' theorem to calculate probabilities and make predictions. Regression models estimate probabilities for the relationship between variables.<br>\nWhen training machine learning models, we draw random samples from the full dataset. This introduces randomness into the learning process, helping avoid overfitting and making the models more robust.<br>\nKey probability distributions used in machine learning include the Gaussian (normal) distribution for modeling continuous variables, the Bernoulli distribution for binary variables, and the Categorical distribution for multi-class outcomes.<br>\nOverall, probability and random variables are essential for developing machine learning models that can handle noise, variability, and uncertainty in real-world data. They provide the mathematical language for \"teaching\" machines from random example\n\n**Example using Bernoulli Distribution**\n\n::: {.cell tags='[]' execution_count=1}\n``` {.python .cell-code}\n# Import numpy for numerical operations\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Define a Bernoulli distribution \np = 0.3\ndist = np.random.binomial(1, p, size=10000)\n\n# Draw some random samples\nsamples = dist\n\n# Calculate mean and standard deviation\nmean = np.mean(samples) \nstd = np.std(samples)\n\n# Plot a histogram\nplt.hist(samples, bins=10)\nplt.title(\"Bernoulli Distribution\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Prob_Theory_Rnd_files/figure-html/cell-2-output-1.png){width=583 height=431}\n:::\n:::\n\n\n**Example using Gaussian Distribution**\n\n::: {.cell tags='[]' execution_count=2}\n``` {.python .cell-code}\n# Define a Gaussian distribution\nmu = 0\nsigma = 1\ndist = np.random.normal(mu, sigma, size=10000) \n\n# Draw random samples \nsamples = dist\n\n# Calculate statistics\nmean = np.mean(samples)\nstd = np.std(samples)\n\n# Plot histogram and probablity distribution function\nfig, ax = plt.subplots(1,2, figsize=(16,6))\nsns.histplot(samples, bins=20, ax=ax[0])\nsns.kdeplot(samples, ax=ax[1])\nax[0].set_xlabel('Histogram')\nax[1].set_xlabel('PDF')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Prob_Theory_Rnd_files/figure-html/cell-3-output-1.png){width=1271 height=503}\n:::\n:::\n\n\n",
    "supporting": [
      "Prob_Theory_Rnd_files"
    ],
    "filters": [],
    "includes": {}
  }
}