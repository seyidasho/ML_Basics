{
  "hash": "ae544e8f1c01584928f2451c3c652966",
  "result": {
    "markdown": "---\ntitle: Linear and Non-linear Regression\nauthor: Seyi Dasho\ndate: December 6 2023\nformat:\n  html:\n    code-fold: true\n---\n\n**Linear and Nonlinear Regression** <br>\nHere is an overview of linear and nonlinear regression in machine learning:<br>\n\n**Linear Regression:** <br>\n- Models the relationship between dependent and independent variables as a linear function. <br>\n- Takes the form:<br> \n  $$y = mx + b$$ <br>\n  $$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\epsilon$$\n\nWhere:\n\n    $y$ is the response/dependent variable\n    $\\beta_0$ is the intercept term\n    $\\beta_1,...,\\beta_p$ are the regression coefficients\n    $x_1,...,x_p$ are the predictor/independent variables\n    $\\epsilon$ is the model error term\n\n- Simple and fast to implement, good exploratory technique. <br>\n- Prone to underfitting and cannot capture nonlinear patterns. <br>\n- Examples: Simple, multiple, polynomial regression. <br>\n\n**Nonlinear Regression:** \n- Models the relationship between variables as a nonlinear function. More flexible and can fit complex patterns.<br>\n- Examples include polynomial, logistic, exponential functions. <br>\n- Takes more computational power and data to fit properly. <br>\n- Examples: Polynomial regression, support vector regression. <br>\n\n- Kernel trick can make linear regression algorithms fit nonlinear patterns. Maps data to higher dimensions.<br>\n- Regularization helps prevent overfitting for both linear and nonlinear models. Penalizes model complexity.<br>\n- Model evaluation metrics like R-squared, MSE help assess fit. Cross-validation reduces overfitting.<br>\n- Feature engineering creates meaningful nonlinear transformations of data for modeling.<br>\nOverall, linear models provide a fast and simple approach, while nonlinear models can capture more complex relationships but require more data and computing power to train effectively. The choice depends on the specific dataset and use case.\n\n::: {.cell tags='[]' execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the diabetes dataset\ndiabetes = load_diabetes()\nX, y = diabetes.data, diabetes.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a linear regression model\nmodel = LinearRegression()\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R-squared: {r2:.2f}')\n\n# Plot the predicted vs. actual values\nplt.scatter(y_test, y_pred)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Linear Regression: Actual vs. Predicted Values')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 2900.19\nR-squared: 0.45\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](L_NL_Regression_files/figure-html/cell-2-output-2.png){width=593 height=449}\n:::\n:::\n\n\n::: {.cell tags='[]' execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Apply Polynomial Regression\ndegree = 2  # Adjust the degree as needed\npoly_features = PolynomialFeatures(degree=degree)\nX_poly_train = poly_features.fit_transform(X_train)\nX_poly_test = poly_features.transform(X_test)\n\nmodel = LinearRegression()\nmodel.fit(X_poly_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_poly_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R-squared: {r2:.2f}')\n\n# Plot the results\nsorted_indices = X_test.argsort(axis=0).flatten()    \nplt.scatter(y_test, y_pred, color='red', label='Predicted Data (Polynomial Regression)')\nplt.xlabel('Feature')\nplt.ylabel('Target')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 3096.03\nR-squared: 0.42\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](L_NL_Regression_files/figure-html/cell-3-output-2.png){width=593 height=429}\n:::\n:::\n\n\n",
    "supporting": [
      "L_NL_Regression_files"
    ],
    "filters": [],
    "includes": {}
  }
}