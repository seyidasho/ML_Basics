[
  {
    "objectID": "Prob_Theory_Rnd.html",
    "href": "Prob_Theory_Rnd.html",
    "title": "Probability Theory and Random Variable",
    "section": "",
    "text": "Probability Theory Probability theory provides the mathematical framework for modeling uncertainty and randomness. It allows us to calculate probabilities, make predictions, and quantify the likelihood of different outcomes. \\[\\displaystyle \\operatorname P(A\\vert B)={\\frac {P(B\\vert A)P(A)}{P(B)}}\\]\nRandom Variables Random variables are key components of probability theory. A random variable represents an outcome that has some randomness or uncertainty associated with it. For example, the weather on a given day can be modeled as a random variable. The probability that random variable X takes on a value in a measurable set S ⊆ E is written as \\[{\\displaystyle \\operatorname {P} (X\\in S)=\\operatorname {P} (\\{\\omega \\in \\Omega \\mid X(\\omega )\\in S\\})}\\] Application in Machine Learning Many machine learning algorithms rely on probability and statistics to model the uncertainty in the data. For example, Naive Bayes classifiers use Bayes’ theorem to calculate probabilities and make predictions. Regression models estimate probabilities for the relationship between variables. When training machine learning models, we draw random samples from the full dataset. This introduces randomness into the learning process, helping avoid overfitting and making the models more robust. Key probability distributions used in machine learning include the Gaussian (normal) distribution for modeling continuous variables, the Bernoulli distribution for binary variables, and the Categorical distribution for multi-class outcomes. Overall, probability and random variables are essential for developing machine learning models that can handle noise, variability, and uncertainty in real-world data. They provide the mathematical language for “teaching” machines from random example\nExample using Bernoulli Distribution\n\n\nCode\n# Import numpy for numerical operations\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Define a Bernoulli distribution \np = 0.3\ndist = np.random.binomial(1, p, size=10000)\n\n# Draw some random samples\nsamples = dist\n\n# Calculate mean and standard deviation\nmean = np.mean(samples) \nstd = np.std(samples)\n\n# Plot a histogram\nplt.hist(samples, bins=10)\nplt.title(\"Bernoulli Distribution\")\nplt.show()\n\n\n\n\n\nExample using Gaussian Distribution\n\n\nCode\n# Define a Gaussian distribution\nmu = 0\nsigma = 1\ndist = np.random.normal(mu, sigma, size=10000) \n\n# Draw random samples \nsamples = dist\n\n# Calculate statistics\nmean = np.mean(samples)\nstd = np.std(samples)\n\n# Plot histogram and probablity distribution function\nfig, ax = plt.subplots(1,2, figsize=(16,6))\nsns.histplot(samples, bins=20, ax=ax[0])\nsns.kdeplot(samples, ax=ax[1])\nax[0].set_xlabel('Histogram')\nax[1].set_xlabel('PDF')\nplt.show()\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "L_NL_Regression.html",
    "href": "L_NL_Regression.html",
    "title": "Linear and Non-linear Regression",
    "section": "",
    "text": "Linear and Nonlinear Regression  Here is an overview of linear and nonlinear regression in machine learning:\nLinear Regression:  - Models the relationship between dependent and independent variables as a linear function.  - Takes the form: \\[y = mx + b\\]  \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\epsilon\\]\nWhere:\n$y$ is the response/dependent variable\n$\\beta_0$ is the intercept term\n$\\beta_1,...,\\beta_p$ are the regression coefficients\n$x_1,...,x_p$ are the predictor/independent variables\n$\\epsilon$ is the model error term\n\nSimple and fast to implement, good exploratory technique. \nProne to underfitting and cannot capture nonlinear patterns. \nExamples: Simple, multiple, polynomial regression. \n\nNonlinear Regression: - Models the relationship between variables as a nonlinear function. More flexible and can fit complex patterns. - Examples include polynomial, logistic, exponential functions.  - Takes more computational power and data to fit properly.  - Examples: Polynomial regression, support vector regression. \n\nKernel trick can make linear regression algorithms fit nonlinear patterns. Maps data to higher dimensions.\nRegularization helps prevent overfitting for both linear and nonlinear models. Penalizes model complexity.\nModel evaluation metrics like R-squared, MSE help assess fit. Cross-validation reduces overfitting.\nFeature engineering creates meaningful nonlinear transformations of data for modeling. Overall, linear models provide a fast and simple approach, while nonlinear models can capture more complex relationships but require more data and computing power to train effectively. The choice depends on the specific dataset and use case.\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the diabetes dataset\ndiabetes = load_diabetes()\nX, y = diabetes.data, diabetes.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a linear regression model\nmodel = LinearRegression()\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R-squared: {r2:.2f}')\n\n# Plot the predicted vs. actual values\nplt.scatter(y_test, y_pred)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Linear Regression: Actual vs. Predicted Values')\nplt.show()\n\n\nMean Squared Error: 2900.19\nR-squared: 0.45\n\n\n\n\n\n\n\nCode\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Apply Polynomial Regression\ndegree = 2  # Adjust the degree as needed\npoly_features = PolynomialFeatures(degree=degree)\nX_poly_train = poly_features.fit_transform(X_train)\nX_poly_test = poly_features.transform(X_test)\n\nmodel = LinearRegression()\nmodel.fit(X_poly_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_poly_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R-squared: {r2:.2f}')\n\n# Plot the results\nsorted_indices = X_test.argsort(axis=0).flatten()    \nplt.scatter(y_test, y_pred, color='red', label='Predicted Data (Polynomial Regression)')\nplt.xlabel('Feature')\nplt.ylabel('Target')\nplt.legend()\nplt.show()\n\n\nMean Squared Error: 3096.03\nR-squared: 0.42\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Classification.html",
    "href": "Classification.html",
    "title": "Classification",
    "section": "",
    "text": "Classification Classification involves predicting a categorical label or class for new observations based on labeled training data.\nCommon Classification Algorithms  i. Logistic Regression: Predicts class probabilities using a logistic function. Popular introductory algorithm. ii. Decision Trees: Makes predictions by branching left or right based on feature values. Interpretable but prone to overfitting. iii. K-Nearest Neighbors: Classifies points based on proximity to labeled examples in feature space. Simple but sensitive to scaling. iv. Support Vector Machines: Finds an optimal hyperplane to separate classes. Powerful but requires tuning. v. Neural Networks: Multi-layer neural net models learn complex class boundaries from features. Very flexible but act as “black boxes”.\nKey steps in classification include: i. Preprocessing data and engineering informative features. ii. Splitting data into train and test sets. iii. Training a model on the training set. iv. Evaluating model accuracy on the test set. v. Using cross-validation techniques to tune hyperparameters and reduce overfitting. vi. Choosing evaluation metrics like accuracy, precision, recall, F1-score based on use case.\n\n\nCode\nimport os\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.datasets import fetch_covtype\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\ncovtype= fetch_covtype() \nX, y = covtype.data, covtype.target\n\n# Split data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train, fit and predict logistic regression model\nlr = LogisticRegression(random_state=42)\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\n# Evaluate the classifiers\naccuracy_lr = accuracy_score(y_test, y_pred_lr)\nconf_matrix_lr = confusion_matrix(y_test, y_pred_lr)\n\n\n# Train, fit and predict Decision Tree model\ndt = DecisionTreeClassifier(random_state=42)\ndt.fit(X_train, y_train)\ny_pred_dt = dt.predict(X_test)\n# Evaluate the classifiers\naccuracy_dt = accuracy_score(y_test, y_pred_dt)\nconf_matrix_dt = confusion_matrix(y_test, y_pred_dt)\n\nprint(f'Decision Tree Accuracy: {accuracy_dt:.2f}')\nprint(f'Logistic Regression Accuracy: {accuracy_lr:.2f}')\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n# Decision Tree Confusion Matrix\nsns.heatmap(conf_matrix_dt, annot=True, fmt='g', cmap='Reds', xticklabels=covtype.target_names, yticklabels=covtype.target_names, ax=axes[0])\naxes[0].set_title('Decision Tree Confusion Matrix')\naxes[0].set_xlabel('Predicted')\naxes[0].set_ylabel('True')\n\n# Random Forest Confusion Matrix\nsns.heatmap(conf_matrix_lr, annot=True, fmt='g', cmap='Reds', xticklabels=covtype.target_names, yticklabels=covtype.target_names, ax=axes[1])\naxes[1].set_title('Logistic Regression Accuracy Matrix')\naxes[1].set_xlabel('Predicted')\naxes[1].set_ylabel('True')\n\nplt.show()\n\n\nDecision Tree Accuracy: 0.93\nLogistic Regression Accuracy: 0.62\n\n\n\n\n\n• This loads the dataset, splits the data into train and test sets, trains a Logistic Regression and a Decision tree models, makes predictions on the test set, and calculates the accuracy. • This shows a basic machine learning workflow for classification in Python - loading data, training a model, making predictions and evaluating performance. The steps can be adapted for other algorithms and datasets.\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Oluwaseyi Dasho, a researcher in the field of Geosciences, I are currently pursuing a PhD and working as a research assistant at Virginia Tech. My specific focus lies in investigating the implications of land subsidence on relative sea level rise and coastal flooding. My work involves delving into the complex interplay between geological processes and their impact on coastal environments.\n\n\n\n Back to top"
  },
  {
    "objectID": "Anomaly_Detection.html",
    "href": "Anomaly_Detection.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Anomaly/outlier detection i. Anomaly detection refers to identifying rare items, events or observations that are significantly different from the majority of the data. These outliers or anomalies may be indicative of some issue or abnormal behavior. ii. Goal is to detect anomalies with high accuracy while minimizing false positives. iii. Applications include fraud detection, network intrusion, breakdown detection in manufacturing, etc. Common approaches:  i. Statistical methods: Declare points outliers if they are far from mean/median or in the tails of a distribution. ii. Distance-based: Find points with low nearest neighbor density as outliers. iii. Classification-based: Train binary classifiers to distinguish between normal and outlier classes. iv. Clustering: Identify small clusters or points far from clusters as anomalous.\n• Unsupervised methods like statistical metrics, isolation forests and local outlier factor do not require labeled data. • Supervised models like SVM, neural networks can be trained if labeled examples of anomalies exist. • Key challenges are class imbalance (far more normal points than anomalies), obtaining labels, and defining anomalies for complex data. • Evaluation metrics include precision, recall, F1-score. Cross-validation helps avoid overfitting. • Overall, anomaly detection is critical for identifying irregularities, errors, novelties and outliers that could indicate important events or issues.\nExample using Local Outlier Factor\n\n\nCode\nfrom sklearn.neighbors import LocalOutlierFactor\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate sample data\nX = 0.3 * np.random.randn(100, 2)\nX = np.r_[X + 2, X - 2]\n\n# \n\n# Fit LOF model\nclf = LocalOutlierFactor()\ny_pred = clf.fit_predict(X)\n\n# Identify outliers\noutlier_idx = np.where(y_pred == -1)\n\n# Plot data\nplt.scatter(X[:,0], X[:,1], s=50)\nplt.scatter(X[outlier_idx,0], X[outlier_idx,1], color='r', s=100)\nplt.title(\"Local Outlier Factor\")\nplt.show()\n\n\n\n\n\n• This fits a LOF model to randomly generated sample data, makes predictions to identify outliers, and plots the outliers in red. • LOF is an unsupervised anomaly detection method that works well with low dimensional data. The key steps - fit model, predict outliers, evaluate results - demonstrate the basic workflow for applying anomaly detection in Python.\nExample using Isolation Forest\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport warnings\nwarnings.filterwarnings('ignore')\n# Load the wine dataset from Scikit-learn\nwine = load_wine()\nX = wine.data\ny = wine.target\n\n# Add outliers to the dataset (for demonstration purposes)\noutliers = np.array([[14, 2, 2, 80, 1, 3, 1, 1, 5, 1, 1, 1, 1000],  # Adding an outlier to the first sample\n                    [12, 1, 2, 10, 2, 2, 1, 2, 3, 1, 2, 1, 50]])  # Adding an outlier to the second sample\nX_with_outliers = np.vstack([X, outliers])\ny_with_outliers = np.concatenate([y, [-1, -1]])  # Adding labels for outliers\n\n# Fit the Isolation Forest model\nmodel = IsolationForest(contamination=0.05)  # Adjust the contamination parameter as needed\nmodel.fit(X_with_outliers)\n\n# Predict anomalies\npredictions = model.predict(X_with_outliers)\n\n# Evaluate the model\nprecision = precision_score(y_with_outliers, predictions, pos_label=-1, average='weighted')\nrecall = recall_score(y_with_outliers, predictions, pos_label=-1, average='weighted')\nf1 = f1_score(y_with_outliers, predictions, pos_label=-1, average='weighted')\n\nprint(f'Precision: {precision:.2f}')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1 Score: {f1:.2f}')\n\n\nPrecision: 0.16\nRecall: 0.38\nF1 Score: 0.22\n\n\n\n\nCode\n# Plot the results\nplt.figure(figsize=(10, 6))\n\n# Plot normal data points\nplt.scatter(X[:, 0], X[:, 1], c='blue', label='Normal Data')\n\n# Plot outliers\nplt.scatter(outliers[:, 0], outliers[:, 1], c='red', label='Outliers', marker='x', s=100)\n\n# Highlight predicted anomalies\nplt.scatter(X_with_outliers[predictions == -1, 0], X_with_outliers[predictions == -1, 1],\n            c='yellow', label='Predicted Anomalies', marker='o')\n\nplt.title('Isolation Forest Anomaly Detection on Scikit-learn Wine Data (with Outliers)')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering  Clustering is an unsupervised learning technique that groups similar data points together based on their features and characteristics. The goal is to find natural groupings within the data.\nClustering algorithms include:  i. K-Means Clustering: Groups data into k clusters by minimizing within-cluster variation. Requires specifying k upfront. ii. Hierarchical Clustering: Builds a hierarchy of clusters in a bottom-up (agglomerative) or top-down (divisive) manner. iii. DBSCAN: Finds dense clusters based on a minimum number of nearby points. Does not require specifying k.\nSteps for clustering generally include:  i. Selecting appropriate features to cluster on  ii. Choosing a clustering algorithm and tuning hyperparameters  iii. Fitting the model to the training data  iv. Evaluating the clusters and analysis results \n\n\nCode\n# Import libraries\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.datasets import make_blobs\n\n\n\n\nCode\n# Generate synthetic data for clustering\nX, y = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=0.60)\n\n# Finding the optimal number of clusters (K)\n# Standardize the features\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(X)\n\n# Use the Elbow Method to find the optimal number of clusters\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\ninertia_values = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(scaled_data)\n    inertia_values.append(kmeans.inertia_)\n\n# Plotting the Elbow Method\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, 11), inertia_values, marker='o')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Inertia')\nplt.title('Elbow Method')\nplt.show()\n\n\n\n\n\n\n\nCode\n# Apply K-means clustering using 4 as optimal cluster number\nkmeans = KMeans(n_clusters=4, random_state=42)\ny_kmeans = kmeans.fit_predict(X)\n\n# Visualize the clusters\nplt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', edgecolor='k')\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, label='Cluster Centers')\nplt.title('K-means Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n\n\n\n\n\nOverall, clustering is a key unsupervised technique for understanding structure in data, summarizing or compressing data, and identifying anomalies. It complements supervised learning methods in machine learning applications.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Basics",
    "section": "",
    "text": "The blog discusses the basics of machine learning.\n\n\n\n Back to top"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Machine learning is a branch of artificial intelligence that focuses on developing algorithms and models that enable computers to learn and make predictions or decisions without being explicitly programmed. The field encompasses a diverse set of techniques that allow systems to automatically improve their performance on a specific task through experience or data.\nKey Concepts:\n\nSupervised Learning:\n\nIn supervised learning, the algorithm is trained on a labeled dataset, where the input data is paired with corresponding output labels.\nThe goal is to learn a mapping function from inputs to outputs, allowing the algorithm to make predictions on new, unseen data.\n\nUnsupervised Learning:\n\nUnsupervised learning deals with unlabeled data, where the algorithm aims to discover hidden patterns or structures within the data.\nClustering and dimensionality reduction are common tasks in unsupervised learning.\n\nTypes of Machine Learning Algorithms:\n\nRegression: Predicting a continuous output based on input features.\nClassification: Assigning input data to predefined categories.\nClustering: Grouping similar data points together.\nDimensionality Reduction: Reducing the number of input features while preserving essential information.\n\nTraining and Testing:\n\nMachine learning models are trained on a subset of the data, and their performance is evaluated on a separate set to assess generalization to new, unseen examples.\n\nFeature Engineering:\n\nSelecting and transforming relevant features in the input data to enhance model performance.\n\nOverfitting and Underfitting:\n\nOverfitting occurs when a model learns the training data too well, including its noise, leading to poor generalization. Underfitting happens when a model is too simplistic to capture the underlying patterns.\n\nEvaluation Metrics:\n\nThe choice of metrics depends on the type of problem. Common metrics include accuracy, precision, recall, F1 score for classification, and mean squared error for regression.\n\nApplications of Machine Learning:\n\nMachine learning is widely used in various domains, including healthcare, finance, natural language processing, image and speech recognition, recommendation systems, and autonomous vehicles.\n\nDeep Learning:\n\nDeep learning is a subfield of machine learning that focuses on neural networks with multiple layers (deep neural networks). It has shown remarkable success in tasks such as image and speech recognition.\n\nEthical Considerations:\n\n\nMachine learning raises ethical concerns related to bias in data, transparency, accountability, and the potential societal impact of automated decision-making.\n\nMachine learning is a dynamic and rapidly evolving field, with ongoing research and advancements continually expanding its applications and capabilities."
  },
  {
    "objectID": "intro.html#introduction-to-machine-learning",
    "href": "intro.html#introduction-to-machine-learning",
    "title": "Introduction",
    "section": "",
    "text": "Machine learning is a branch of artificial intelligence that focuses on developing algorithms and models that enable computers to learn and make predictions or decisions without being explicitly programmed. The field encompasses a diverse set of techniques that allow systems to automatically improve their performance on a specific task through experience or data.\nKey Concepts:\n\nSupervised Learning:\n\nIn supervised learning, the algorithm is trained on a labeled dataset, where the input data is paired with corresponding output labels.\nThe goal is to learn a mapping function from inputs to outputs, allowing the algorithm to make predictions on new, unseen data.\n\nUnsupervised Learning:\n\nUnsupervised learning deals with unlabeled data, where the algorithm aims to discover hidden patterns or structures within the data.\nClustering and dimensionality reduction are common tasks in unsupervised learning.\n\nTypes of Machine Learning Algorithms:\n\nRegression: Predicting a continuous output based on input features.\nClassification: Assigning input data to predefined categories.\nClustering: Grouping similar data points together.\nDimensionality Reduction: Reducing the number of input features while preserving essential information.\n\nTraining and Testing:\n\nMachine learning models are trained on a subset of the data, and their performance is evaluated on a separate set to assess generalization to new, unseen examples.\n\nFeature Engineering:\n\nSelecting and transforming relevant features in the input data to enhance model performance.\n\nOverfitting and Underfitting:\n\nOverfitting occurs when a model learns the training data too well, including its noise, leading to poor generalization. Underfitting happens when a model is too simplistic to capture the underlying patterns.\n\nEvaluation Metrics:\n\nThe choice of metrics depends on the type of problem. Common metrics include accuracy, precision, recall, F1 score for classification, and mean squared error for regression.\n\nApplications of Machine Learning:\n\nMachine learning is widely used in various domains, including healthcare, finance, natural language processing, image and speech recognition, recommendation systems, and autonomous vehicles.\n\nDeep Learning:\n\nDeep learning is a subfield of machine learning that focuses on neural networks with multiple layers (deep neural networks). It has shown remarkable success in tasks such as image and speech recognition.\n\nEthical Considerations:\n\n\nMachine learning raises ethical concerns related to bias in data, transparency, accountability, and the potential societal impact of automated decision-making.\n\nMachine learning is a dynamic and rapidly evolving field, with ongoing research and advancements continually expanding its applications and capabilities."
  }
]