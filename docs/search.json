[
  {
    "objectID": "Prob_Theory_Rnd.html",
    "href": "Prob_Theory_Rnd.html",
    "title": "Probability Theory and Random Variable",
    "section": "",
    "text": "Probability Theory Probability theory provides the mathematical framework for modeling uncertainty and randomness. It allows us to calculate probabilities, make predictions, and quantify the likelihood of different outcomes.\nRandom Variables Random variables are key components of probability theory. A random variable represents an outcome that has some randomness or uncertainty associated with it. For example, the weather on a given day can be modeled as a random variable.\nApplication in Machine Learning Many machine learning algorithms rely on probability and statistics to model the uncertainty in the data. For example, Naive Bayes classifiers use Bayes’ theorem to calculate probabilities and make predictions. Regression models estimate probabilities for the relationship between variables. When training machine learning models, we draw random samples from the full dataset. This introduces randomness into the learning process, helping avoid overfitting and making the models more robust. Key probability distributions used in machine learning include the Gaussian (normal) distribution for modeling continuous variables, the Bernoulli distribution for binary variables, and the Categorical distribution for multi-class outcomes. Overall, probability and random variables are essential for developing machine learning models that can handle noise, variability, and uncertainty in real-world data. They provide the mathematical language for “teaching” machines from random example\nExample using Bernoulli Distribution\n\n\nCode\n# Import numpy for numerical operations\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define a Bernoulli distribution \np = 0.3\ndist = np.random.binomial(1, p, size=10000)\n\n# Draw some random samples\nsamples = dist\n\n# Calculate mean and standard deviation\nmean = np.mean(samples) \nstd = np.std(samples)\n\n# Plot a histogram\nplt.hist(samples, bins=10)\nplt.title(\"Bernoulli Distribution\")\nplt.show()\n\n\n\n\n\nExample using Gaussian Distribution\n\n\nCode\n# Define a Gaussian distribution\nmu = 0\nsigma = 1\ndist = np.random.normal(mu, sigma, size=10000) \n\n# Draw random samples \nsamples = dist\n\n# Calculate statistics\nmean = np.mean(samples)\nstd = np.std(samples)\n\n# Plot histogram\nplt.hist(samples, bins=20)\nplt.title(\"Gaussian Distribution\")\n\nplt.show()"
  },
  {
    "objectID": "L_NL_Regression.html",
    "href": "L_NL_Regression.html",
    "title": "Linear and Non-linear Regression",
    "section": "",
    "text": "Linear and Nonlinear Regression  Here is an overview of linear and nonlinear regression in machine learning:\nLinear Regression:  - Models the relationship between dependent and independent variables as a linear function.  - Takes the form: y = mx + b  - Simple and fast to implement, good exploratory technique.  - Prone to underfitting and cannot capture nonlinear patterns.  - Examples: Simple, multiple, polynomial regression. \nNonlinear Regression: - Models the relationship between variables as a nonlinear function. More flexible and can fit complex patterns. - Examples include polynomial, logistic, exponential functions.  - Takes more computational power and data to fit properly.  - Examples: Polynomial regression, support vector regression. \n\nKernel trick can make linear regression algorithms fit nonlinear patterns. Maps data to higher dimensions.\nRegularization helps prevent overfitting for both linear and nonlinear models. Penalizes model complexity.\nModel evaluation metrics like R-squared, MSE help assess fit. Cross-validation reduces overfitting.\nFeature engineering creates meaningful nonlinear transformations of data for modeling. Overall, linear models provide a fast and simple approach, while nonlinear models can capture more complex relationships but require more data and computing power to train effectively. The choice depends on the specific dataset and use case.\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the diabetes dataset\ndiabetes = load_diabetes()\nX, y = diabetes.data, diabetes.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a linear regression model\nmodel = LinearRegression()\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R-squared: {r2:.2f}')\n\n# Plot the predicted vs. actual values\nplt.scatter(y_test, y_pred)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Linear Regression: Actual vs. Predicted Values')\nplt.show()\n\n\nMean Squared Error: 2900.19\nR-squared: 0.45\n\n\n\n\n\n\n\nCode\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Apply Polynomial Regression\ndegree = 2  # Adjust the degree as needed\npoly_features = PolynomialFeatures(degree=degree)\nX_poly_train = poly_features.fit_transform(X_train)\nX_poly_test = poly_features.transform(X_test)\n\nmodel = LinearRegression()\nmodel.fit(X_poly_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_poly_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R-squared: {r2:.2f}')\n\n# Plot the results\nsorted_indices = X_test.argsort(axis=0).flatten()    \nplt.scatter(y_test, y_pred, color='red', label='Predicted Data (Polynomial Regression)')\nplt.xlabel('Feature')\nplt.ylabel('Target')\nplt.legend()\nplt.show()\n\n\nMean Squared Error: 3096.03\nR-squared: 0.42"
  },
  {
    "objectID": "Classification.html",
    "href": "Classification.html",
    "title": "Classification",
    "section": "",
    "text": "Classification Classification involves predicting a categorical label or class for new observations based on labeled training data.\nCommon Classification Algorithms  o Logistic Regression: Predicts class probabilities using a logistic function. Popular introductory algorithm. o Decision Trees: Makes predictions by branching left or right based on feature values. Interpretable but prone to overfitting. o K-Nearest Neighbors: Classifies points based on proximity to labeled examples in feature space. Simple but sensitive to scaling. o Support Vector Machines: Finds an optimal hyperplane to separate classes. Powerful but requires tuning. o Neural Networks: Multi-layer neural net models learn complex class boundaries from features. Very flexible but act as “black boxes”.\nKey steps in classification include: o Preprocessing data and engineering informative features. o Splitting data into train and test sets. o Training a model on the training set. o Evaluating model accuracy on the test set. o Using cross-validation techniques to tune hyperparameters and reduce overfitting. o Choosing evaluation metrics like accuracy, precision, recall, F1-score based on use case.\n\n\nCode\nimport os\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.datasets import fetch_covtype\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\ncovtype= fetch_covtype() \nX, y = covtype.data, covtype.target\n\n# Split data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train, fit and predict logistic regression model\nlr = LogisticRegression(random_state=42)\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\n# Evaluate the classifiers\naccuracy_lr = accuracy_score(y_test, y_pred_lr)\nconf_matrix_lr = confusion_matrix(y_test, y_pred_lr)\n\n\n# Train, fit and predict Decision Tree model\ndt = DecisionTreeClassifier(random_state=42)\ndt.fit(X_train, y_train)\ny_pred_dt = dt.predict(X_test)\n# Evaluate the classifiers\naccuracy_dt = accuracy_score(y_test, y_pred_dt)\nconf_matrix_dt = confusion_matrix(y_test, y_pred_dt)\n\nprint(f'Decision Tree Accuracy: {accuracy_dt:.2f}')\nprint(f'Logistic Regression Accuracy: {accuracy_lr:.2f}')\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n# Decision Tree Confusion Matrix\nsns.heatmap(conf_matrix_dt, annot=True, fmt='g', cmap='Reds', xticklabels=covtype.target_names, yticklabels=covtype.target_names, ax=axes[0])\naxes[0].set_title('Decision Tree Confusion Matrix')\naxes[0].set_xlabel('Predicted')\naxes[0].set_ylabel('True')\n\n# Random Forest Confusion Matrix\nsns.heatmap(conf_matrix_lr, annot=True, fmt='g', cmap='Reds', xticklabels=covtype.target_names, yticklabels=covtype.target_names, ax=axes[1])\naxes[1].set_title('Logistic Regression Accuracy Matrix')\naxes[1].set_xlabel('Predicted')\naxes[1].set_ylabel('True')\n\nplt.show()\n\n\nDecision Tree Accuracy: 0.93\nLogistic Regression Accuracy: 0.62\n\n\n\n\n\n• This loads the dataset, splits the data into train and test sets, trains a Logistic Regression and a Decision tree models, makes predictions on the test set, and calculates the accuracy. • This shows a basic machine learning workflow for classification in Python - loading data, training a model, making predictions and evaluating performance. The steps can be adapted for other algorithms and datasets."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n2"
  },
  {
    "objectID": "Anomaly_Detection.html",
    "href": "Anomaly_Detection.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Anomaly/outlier detection • Anomaly detection refers to identifying rare items, events or observations that are significantly different from the majority of the data. These outliers or anomalies may be indicative of some issue or abnormal behavior. • Goal is to detect anomalies with high accuracy while minimizing false positives. • Applications include fraud detection, network intrusion, breakdown detection in manufacturing, etc. Common approaches:  o Statistical methods: Declare points outliers if they are far from mean/median or in the tails of a distribution. o Distance-based: Find points with low nearest neighbor density as outliers. o Classification-based: Train binary classifiers to distinguish between normal and outlier classes. o Clustering: Identify small clusters or points far from clusters as anomalous. • Unsupervised methods like statistical metrics, isolation forests and local outlier factor do not require labeled data. • Supervised models like SVM, neural networks can be trained if labeled examples of anomalies exist. • Key challenges are class imbalance (far more normal points than anomalies), obtaining labels, and defining anomalies for complex data. • Evaluation metrics include precision, recall, F1-score. Cross-validation helps avoid overfitting. • Overall, anomaly detection is critical for identifying irregularities, errors, novelties and outliers that could indicate important events or issues.\nExample using Local Outlier Factor\n\n\nCode\nfrom sklearn.neighbors import LocalOutlierFactor\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate sample data\nX = 0.3 * np.random.randn(100, 2)\nX = np.r_[X + 2, X - 2]\n\n# \n\n# Fit LOF model\nclf = LocalOutlierFactor()\ny_pred = clf.fit_predict(X)\n\n# Identify outliers\noutlier_idx = np.where(y_pred == -1)\n\n# Plot data\nplt.scatter(X[:,0], X[:,1], s=50)\nplt.scatter(X[outlier_idx,0], X[outlier_idx,1], color='r', s=100)\nplt.title(\"Local Outlier Factor\")\nplt.show()\n\n\n\n\n\n• This fits a LOF model to randomly generated sample data, makes predictions to identify outliers, and plots the outliers in red. • LOF is an unsupervised anomaly detection method that works well with low dimensional data. The key steps - fit model, predict outliers, evaluate results - demonstrate the basic workflow for applying anomaly detection in Python.\nExample using Isolation Forest\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport warnings\nwarnings.filterwarnings('ignore')\n# Load the wine dataset from Scikit-learn\nwine = load_wine()\nX = wine.data\ny = wine.target\n\n# Add outliers to the dataset (for demonstration purposes)\noutliers = np.array([[14, 2, 2, 80, 1, 3, 1, 1, 5, 1, 1, 1, 1000],  # Adding an outlier to the first sample\n                    [12, 1, 2, 10, 2, 2, 1, 2, 3, 1, 2, 1, 50]])  # Adding an outlier to the second sample\nX_with_outliers = np.vstack([X, outliers])\ny_with_outliers = np.concatenate([y, [-1, -1]])  # Adding labels for outliers\n\n# Fit the Isolation Forest model\nmodel = IsolationForest(contamination=0.05)  # Adjust the contamination parameter as needed\nmodel.fit(X_with_outliers)\n\n# Predict anomalies\npredictions = model.predict(X_with_outliers)\n\n# Evaluate the model\nprecision = precision_score(y_with_outliers, predictions, pos_label=-1, average='weighted')\nrecall = recall_score(y_with_outliers, predictions, pos_label=-1, average='weighted')\nf1 = f1_score(y_with_outliers, predictions, pos_label=-1, average='weighted')\n\nprint(f'Precision: {precision:.2f}')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1 Score: {f1:.2f}')\n\n\nPrecision: 0.15\nRecall: 0.38\nF1 Score: 0.22\n\n\n\n\nCode\n# Plot the results\nplt.figure(figsize=(10, 6))\n\n# Plot normal data points\nplt.scatter(X[:, 0], X[:, 1], c='blue', label='Normal Data')\n\n# Plot outliers\nplt.scatter(outliers[:, 0], outliers[:, 1], c='red', label='Outliers', marker='x', s=100)\n\n# Highlight predicted anomalies\nplt.scatter(X_with_outliers[predictions == -1, 0], X_with_outliers[predictions == -1, 1],\n            c='yellow', label='Predicted Anomalies', marker='o')\n\nplt.title('Isolation Forest Anomaly Detection on Scikit-learn Wine Data (with Outliers)')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "CLUSTERING  • Clustering is an unsupervised learning technique that groups similar data points together based on their features and characteristics. The goal is to find natural groupings within the data.\nClustering algorithms include:  o K-Means Clustering: Groups data into k clusters by minimizing within-cluster variation. Requires specifying k upfront. o Hierarchical Clustering: Builds a hierarchy of clusters in a bottom-up (agglomerative) or top-down (divisive) manner. o DBSCAN: Finds dense clusters based on a minimum number of nearby points. Does not require specifying k.\nSteps for clustering generally include:  1. Selecting appropriate features to cluster on  2. Choosing a clustering algorithm and tuning hyperparameters  3. Fitting the model to the training data  4. Evaluating the clusters and analysis results \n\n\nCode\n# Import libraries\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.datasets import make_blobs\n\n\n\n\nCode\n# Generate synthetic data for clustering\nX, y = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=0.60)\n\n# Finding the optimal number of clusters (K)\n# Standardize the features\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(X)\n\n# Use the Elbow Method to find the optimal number of clusters\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\ninertia_values = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(scaled_data)\n    inertia_values.append(kmeans.inertia_)\n\n# Plotting the Elbow Method\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, 11), inertia_values, marker='o')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Inertia')\nplt.title('Elbow Method')\nplt.show()\n\n\n\n\n\n\n\nCode\n# Apply K-means clustering using 4 as optimal cluster number\nkmeans = KMeans(n_clusters=4, random_state=42)\ny_kmeans = kmeans.fit_predict(X)\n\n# Visualize the clusters\nplt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', edgecolor='k')\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, label='Cluster Centers')\nplt.title('K-means Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n\n\n\n\n\n• Overall, clustering is a key unsupervised technique for understanding structure in data, summarizing or compressing data, and identifying anomalies. It complements supervised learning methods in machine learning applications."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML_Basics",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n2"
  }
]